hadoop:
  enabled: true
  webhdfs:
    enabled: true

fluentd:
  sinks:
    hdfs: true

  configMaps:
    output.conf: |
      # Store Data in Elasticsearch and S3

      <match **>
        @type copy
        deep_copy true
        <store>
          @id elasticsearch
          @type elasticsearch
          @log_level info
          include_tag_key true
          # Replace with the host/port to your Elasticsearch cluster.
          host "#{ENV['OUTPUT_HOST']}"
          port "#{ENV['OUTPUT_PORT']}"
          scheme "#{ENV['OUTPUT_SCHEME']}"
          ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
          logstash_format true
          <buffer time,tag>
            @type file
            timekey 300
            path /var/log/fluentd-buffers/kubernetes.elastic.buffer
            flush_mode interval
            retry_type exponential_backoff
            flush_thread_count 2
            flush_interval 5s
            retry_forever
            retry_max_interval 30
            chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
            queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
            overflow_action block
          </buffer>
        </store>
        <store>
          @type webhdfs
          host efk-hadoop-hdfs-nn
          port 50070
          path /logs/${tag}/access.log.%Y%m%d_%H.log
          <buffer time,tag>
            @type   file
            path    /var/log/fluentd/buffer
            timekey 3h
            timekey_use_utc true
          </buffer>
          <inject>
            time_key  time
            time_type unixtime
          </inject>
          <format>
            @type json
          </format>
        </store>
      </match>

