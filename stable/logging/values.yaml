# default values for all charts
nameOverride: &nameOverride ""
fullnameOverride: &fullnameOverride ""

resources: &resources {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #  cpu: 100m
  #  memory: 128Mi
  # requests:
  #  cpu: 100m
#  memory: 128Mi

nodeSelector: &nodeSelector {}

tolerations: &tolerations []

affinity: &affinity {}

rbac: &RBACDefaults
  create: true


# config for kibana dashboard: https://github.com/elastic/helm-charts/tree/master/kibana
kibana:
  enabled: true

  # common values
  nameOverride: *nameOverride
  fullnameOverride: *fullnameOverride
  nodeSelector: *nodeSelector
  tolerations: *tolerations
  affinity: *affinity
  resources: *resources
  rbac: *RBACDefaults


# config for elasticsearch: https://github.com/elastic/helm-charts/tree/master/kibana
elasticsearch:
  enabled: true
  replicas: 2
  antiAffinity: soft

  # common values
  nameOverride: *nameOverride
  fullnameOverride: *fullnameOverride
  nodeSelector: *nodeSelector
  tolerations: *tolerations
  affinity: *affinity
  resources: *resources
  rbac: *RBACDefaults

# config for fluentd aggregator: https://github.com/helm/charts/tree/master/stable/fluentd
fluentd:
  enabled: true

  # common values
  nameOverride: *nameOverride
  fullnameOverride: *fullnameOverride
  nodeSelector: *nodeSelector
  tolerations: *tolerations
  affinity: *affinity
  resources: *resources
  rbac: *RBACDefaults

  service:
    annotations: {}
    type: ClusterIP
    ports:
      - name: "fluent-input"
        protocol: TCP
        containerPort: 24224
  configMaps:
    general.conf: |
      # Prevent fluentd from handling records containing its own logs. Otherwise
      # it can lead to an infinite loop, when error in sending one message generates
      # another message which also fails to be sent and so on.
      <match fluentd.**>
        @type null
      </match>
      # Used for health checking
      <source>
        @type http
        port 9880
        bind 0.0.0.0
      </source>
      # Emits internal metrics to every minute, and also exposes them on port
      # 24220. Useful for determining if an output plugin is retryring/erroring,
      # or determining the buffer queue length.
      <source>
        @type monitor_agent
        bind 0.0.0.0
        port 24220
        tag fluentd.monitor.metrics
      </source>

      <match kubernetes.**>
        @type rewrite_tag_filter
        # for spark
        <rule>
          key $.kubernetes
          pattern container_name\"=>"([^\"]*)\".*namespace_name\"=>\"([^\"]*)\".*pod_name\"=>\"([^\"]*)\".*pod_id\"=>\"([^\"]*)\".*"spark-role"=>"([^\"]*)\".*"sparkoperator_k8s_io\/app-name"=>"([^\"]*)\".*"sparkoperator_k8s_io\/submission-id"=>"([^\"]*)\"
          tag $2/$6-$7/$5
        </rule>
        # for other apps
        <rule>
          key $.kubernetes
          pattern container_name\"=>"([^\"]*)\".*namespace_name\"=>\"([^\"]*)\".*pod_name\"=>\"([^\"]*)\".*pod_id\"=>\"([^\"]*)\"
          tag $2/$3/$1-$4
        </rule>
      </match>
    system.conf: |-
      <system>
        root_dir /tmp/fluentd-buffers/
      </system>
    output.conf: |
      # Store Data in Elasticsearch and S3

      <match kubernetes.**>
        @type rewrite_tag_filter
        <rule>
          key $.kubernetes
          pattern container_name\"=>"([^\"]*)\".*namespace_name\"=>\"([^\"]*)\".*pod_name\"=>\"([^\"]*)\".*pod_id\"=>\"([^\"]*)\"
          tag $2/$3/$1-$4
        </rule>
      </match>

      <match **>
        @type copy
        deep_copy true
        <store>
          @id elasticsearch
          @type elasticsearch
          @log_level info
          include_tag_key true
          # Replace with the host/port to your Elasticsearch cluster.
          host "#{ENV['OUTPUT_HOST']}"
          port "#{ENV['OUTPUT_PORT']}"
          scheme "#{ENV['OUTPUT_SCHEME']}"
          ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
          logstash_format true
          <buffer time,tag>
            @type file
            timekey 300
            path /var/log/fluentd-buffers/kubernetes.elastic.buffer
            flush_mode interval
            retry_type exponential_backoff
            flush_thread_count 2
            flush_interval 5s
            retry_forever
            retry_max_interval 30
            chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
            queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
            overflow_action block
          </buffer>
        </store>
      </match>

  sinks:
    s3: false
    hdfs: false
    pv: false

  persistence:
    enabled: true
    size: 20Gi

  autoscaling:
    enabled: false


  secrets:
    s3:
      access_key_id:
      secret_access_key:
      bucket:
      region:

  plugins:
    enabled: true
    pluginsList:
      - "fluent-plugin-s3"
      - "fluent-plugin-forest"
      - "fluent-plugin-rewrite-tag-filter"
      - "fluent-plugin-webhdfs"

#  extraEnvVars:
#    - name: S3_KEY_ID
#      value: <aws s3 secret key id>
#    - name: S3_SECRET_KEY
#      value: <aws s3 secret key>
#    - name: S3_BUCKET
#      value: <bucket name>
#    - name: S3_REGION
#      value: <bucket region>

  output:
    host: elasticsearch-master

# config for fluentd-forwarder daemon-set
fluentdForwarder:
  enabled: true

  # common values
  nameOverride: *nameOverride
  fullnameOverride: *fullnameOverride
  nodeSelector: *nodeSelector
  affinity: *affinity
  rbac: *RBACDefaults

  resources:
    limits:
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 200Mi

  containerName: fluentd-forwarder
  image:
    name: fluent/fluentd-kubernetes-daemonset
    tag: v1.8.1-debian-forward-1.0
    pullPolicy: "IfNotPresent"

  tolerations:
    - key: node-role.kubernetes.io/master
      effect: NoSchedule
    - operator: "Exists"

  forward:
    host: efk-fluentd.qubole-logging.svc.cluster.local
    port : 24224

  extraEnvVars:

  serviceAccount:
    create: true

  config:
    disable.conf: |
    fluent.conf: |
      @include "#{ENV['FLUENTD_SYSTEMD_CONF'] || 'systemd'}.conf"
      @include "#{ENV['FLUENTD_PROMETHEUS_CONF'] || 'prometheus'}.conf"
      @include kubernetes.conf
      @include conf.d/*.conf

      <match **>
        @type forward
        @id out_fwd
        @log_level info
        <server>
          host "#{ENV['FLUENT_FOWARD_HOST']}"
          port "#{ENV['FLUENT_FOWARD_PORT']}"
        </server>
      </match>
    kubernetes.conf: |
      <match fluent.**>
        @type null
      </match>

      <source>
        @type tail
        @id in_tail_container_logs
        path /var/log/containers/*.log
        exclude_path ["/var/log/containers/efk-fluentd*"]
        pos_file /var/log/fluentd-containers.log.pos
        tag "#{ENV['FLUENT_CONTAINER_TAIL_TAG'] || 'kubernetes.*'}"
        read_from_head true
        <parse>
          @type "#{ENV['FLUENT_CONTAINER_TAIL_PARSER_TYPE'] || 'json'}"
          time_format %Y-%m-%dT%H:%M:%S.%NZ
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_minion
        path /var/log/salt/minion
        pos_file /var/log/fluentd-salt.pos
        tag salt
        <parse>
          @type regexp
          expression /^(?<time>[^ ]* [^ ,]*)[^\[]*\[[^\]]*\]\[(?<severity>[^ \]]*) *\] (?<message>.*)$/
          time_format %Y-%m-%d %H:%M:%S
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_startupscript
        path /var/log/startupscript.log
        pos_file /var/log/fluentd-startupscript.log.pos
        tag startupscript
        <parse>
          @type syslog
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_docker
        path /var/log/docker.log
        pos_file /var/log/fluentd-docker.log.pos
        tag docker
        <parse>
          @type regexp
          expression /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_etcd
        path /var/log/etcd.log
        pos_file /var/log/fluentd-etcd.log.pos
        tag etcd
        <parse>
          @type none
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_kubelet
        multiline_flush_interval 5s
        path /var/log/kubelet.log
        pos_file /var/log/fluentd-kubelet.log.pos
        tag kubelet
        <parse>
          @type kubernetes
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_kube_proxy
        multiline_flush_interval 5s
        path /var/log/kube-proxy.log
        pos_file /var/log/fluentd-kube-proxy.log.pos
        tag kube-proxy
        <parse>
          @type kubernetes
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_kube_apiserver
        multiline_flush_interval 5s
        path /var/log/kube-apiserver.log
        pos_file /var/log/fluentd-kube-apiserver.log.pos
        tag kube-apiserver
        <parse>
          @type kubernetes
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_kube_controller_manager
        multiline_flush_interval 5s
        path /var/log/kube-controller-manager.log
        pos_file /var/log/fluentd-kube-controller-manager.log.pos
        tag kube-controller-manager
        <parse>
          @type kubernetes
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_kube_scheduler
        multiline_flush_interval 5s
        path /var/log/kube-scheduler.log
        pos_file /var/log/fluentd-kube-scheduler.log.pos
        tag kube-scheduler
        <parse>
          @type kubernetes
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_rescheduler
        multiline_flush_interval 5s
        path /var/log/rescheduler.log
        pos_file /var/log/fluentd-rescheduler.log.pos
        tag rescheduler
        <parse>
          @type kubernetes
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_glbc
        multiline_flush_interval 5s
        path /var/log/glbc.log
        pos_file /var/log/fluentd-glbc.log.pos
        tag glbc
        <parse>
          @type kubernetes
        </parse>
      </source>

      <source>
        @type tail
        @id in_tail_cluster_autoscaler
        multiline_flush_interval 5s
        path /var/log/cluster-autoscaler.log
        pos_file /var/log/fluentd-cluster-autoscaler.log.pos
        tag cluster-autoscaler
        <parse>
          @type kubernetes
        </parse>
      </source>

      # Example:
      # 2017-02-09T00:15:57.992775796Z AUDIT: id="90c73c7c-97d6-4b65-9461-f94606ff825f" ip="104.132.1.72" method="GET" user="kubecfg" as="<self>" asgroups="<lookup>" namespace="default" uri="/api/v1/namespaces/default/pods"
      # 2017-02-09T00:15:57.993528822Z AUDIT: id="90c73c7c-97d6-4b65-9461-f94606ff825f" response="200"
      <source>
        @type tail
        @id in_tail_kube_apiserver_audit
        multiline_flush_interval 5s
        path /var/log/kubernetes/kube-apiserver-audit.log
        pos_file /var/log/kube-apiserver-audit.log.pos
        tag kube-apiserver-audit
        <parse>
          @type multiline
          format_firstline /^\S+\s+AUDIT:/
          # Fields must be explicitly captured by name to be parsed into the record.
          # Fields may not always be present, and order may change, so this just looks
          # for a list of key="\"quoted\" value" pairs separated by spaces.
          # Unknown fields are ignored.
          # Note: We can't separate query/response lines as format1/format2 because
          #       they don't always come one after the other for a given query.
          format1 /^(?<time>\S+) AUDIT:(?: (?:id="(?<id>(?:[^"\\]|\\.)*)"|ip="(?<ip>(?:[^"\\]|\\.)*)"|method="(?<method>(?:[^"\\]|\\.)*)"|user="(?<user>(?:[^"\\]|\\.)*)"|groups="(?<groups>(?:[^"\\]|\\.)*)"|as="(?<as>(?:[^"\\]|\\.)*)"|asgroups="(?<asgroups>(?:[^"\\]|\\.)*)"|namespace="(?<namespace>(?:[^"\\]|\\.)*)"|uri="(?<uri>(?:[^"\\]|\\.)*)"|response="(?<response>(?:[^"\\]|\\.)*)"|\w+="(?:[^"\\]|\\.)*"))*/
          time_format %Y-%m-%dT%T.%L%Z
        </parse>
      </source>

      <filter kubernetes.**>
        @type kubernetes_metadata
        @id filter_kube_metadata
        kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
        verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
        ca_file "#{ENV['KUBERNETES_CA_FILE']}"
      </filter>
    prometheus.conf: |
      # Prometheus metric exposed on 0.0.0.0:24231/metrics
      <source>
        @type prometheus
        bind "#{ENV['FLUENTD_PROMETHEUS_BIND'] || '0.0.0.0'}"
        port "#{ENV['FLUENTD_PROMETHEUS_PORT'] || '24231'}"
        metrics_path "#{ENV['FLUENTD_PROMETHEUS_PATH'] || '/metrics'}"
      </source>

      <source>
        @type prometheus_output_monitor
      </source>
    systemd.conf: |
      # Logs from systemd-journal for interesting services.
      <source>
        @type systemd
        @id in_systemd_kubelet
        matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
        <storage>
          @type local
          persistent true
          path /var/log/fluentd-journald-kubelet-cursor.json
        </storage>
        read_from_head true
        tag kubelet
      </source>

      # Logs from docker-systemd
      <source>
        @type systemd
        @id in_systemd_docker
        matches [{ "_SYSTEMD_UNIT": "docker.service" }]
        <storage>
          @type local
          persistent true
          path /var/log/fluentd-journald-docker-cursor.json
        </storage>
        read_from_head true
        tag docker.systemd
      </source>

      # Logs from systemd-journal for interesting services.
      <source>
        @type systemd
        @id in_systemd_bootkube
        matches [{ "_SYSTEMD_UNIT": "bootkube.service" }]
        <storage>
          @type local
          persistent true
          path /var/log/fluentd-journald-bootkube-cursor.json
        </storage>
        read_from_head true
        tag bootkube
      </source>

