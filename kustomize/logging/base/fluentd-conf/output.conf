# Store Data in Elasticsearch and S3

<match kubernetes.**>
  @type rewrite_tag_filter
  <rule>
    key $.kubernetes
    pattern container_name\"=>"([^\"]*)\".*namespace_name\"=>\"([^\"]*)\".*pod_name\"=>\"([^\"]*)\".*pod_id\"=>\"([^\"]*)\"
    tag $2/$3/$1-$4
  </rule>
</match>

<match **>
  @type copy
  deep_copy true
  <store>
    @id elasticsearch
    @type elasticsearch
    @log_level info
    include_tag_key true
    # Replace with the host/port to your Elasticsearch cluster.
    host "#{ENV['OUTPUT_HOST']}"
    port "#{ENV['OUTPUT_PORT']}"
    scheme "#{ENV['OUTPUT_SCHEME']}"
    ssl_version "#{ENV['OUTPUT_SSL_VERSION']}"
    logstash_format true
    <buffer time,tag>
      @type file
      timekey 300
      path /var/log/fluentd-buffers/kubernetes.elastic.buffer
      flush_mode interval
      retry_type exponential_backoff
      flush_thread_count 2
      flush_interval 5s
      retry_forever
      retry_max_interval 30
      chunk_limit_size "#{ENV['OUTPUT_BUFFER_CHUNK_LIMIT']}"
      queue_limit_length "#{ENV['OUTPUT_BUFFER_QUEUE_LIMIT']}"
      overflow_action block
    </buffer>
  </store>
</match>